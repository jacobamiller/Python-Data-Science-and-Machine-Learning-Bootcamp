{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___\n",
    "# MNIST Multi-Layer Perceptron\n",
    "\n",
    "In this lecture we will build out a Multi Layer Perceptron model to try to classify hand written digits using TensorFlow (a very famous example).\n",
    "\n",
    "Keep in mind that no single lecture (or course!) can cover the vastness that is Deep Learning, I would highly suggest reading MIT's [Deep Learning](http://www.deeplearningbook.org/) textbook for more information on these topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "We will be using the famous MNIST data set of [handwritten digits](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "The images which we will be working with are black and white images of size 28 x 28 pixels, or 784 pixels total. Our features will be the pixel values for each pixel. Either the pixel is \"white\" (blank with a 0), or there is some pixel value. \n",
    "\n",
    "We will try to correctly predict what number is written down based solely on the image data in the form of an array. This type of problem (Image Recognition) is a great use case for Deep Learning Methods!\n",
    "\n",
    "This data is to Deep Learning what the iris data set is to typical machine learning algorithms.  \n",
    "\n",
    "Let's get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HIDE ALL WARNINGS \n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format\n",
    "\n",
    "The data is stored in a vector format, although the original data was a 2-dimensional matirx with values representing how much pigment was at a certain location. Let's explore this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mnist.train.images[0]\n",
    "mnist.train.images[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = mnist.train.images[2].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a6393f9550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADWNJREFUeJzt3X+oXPWZx/HPx2zrj6SCIdc02OjtFtENxk3XIQou4lIt6VKIBaPJHyULoanY4Bb6x0pAq4gikrYKSjXdhEZIbQuJa/zBboMsSQulOP6gMRvXiNxtchOSe1FpKmKJefaPe1Ju450zNzNn5sy9z/sFYWbOc86ch6Ofe2bmO3O+jggByOecuhsAUA/CDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqb/p584WLFgQw8PD/dwlkMrIyIjGx8c9nXW7Cr/tFZIekzRH0r9HxMNl6w8PD6vZbHazSwAlGo3GtNft+GW/7TmSnpD0NUlLJK2xvaTT5wPQX928518u6Z2IeDci/izp55JWVtMWgF7rJvyXSDo06fHhYtlfsb3edtN2c2xsrIvdAahSN+Gf6kOFT/0+OCI2R0QjIhpDQ0Nd7A5AlboJ/2FJiyc9/oKkI921A6Bfugn/K5Iut/1F25+VtFrSrmraAtBrHQ/1RcRJ2xsk/Zcmhvq2RsT+yjoD0FNdjfNHxEuSXqqoFwB9xNd7gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKqrWXptj0g6IekTSScjolFFUwB6r6vwF/4pIsYreB4AfcTLfiCpbsMfkn5l+1Xb66toCEB/dPuy//qIOGL7Ykm7bb8VEXsnr1D8UVgvSZdeemmXuwNQla7O/BFxpLg9LulZScunWGdzRDQiojE0NNTN7gBUqOPw255r+3On70v6qqQ3q2oMQG9187J/oaRnbZ9+np9FxH9W0hWAnus4/BHxrqS/r7AXAH3EUB+QFOEHkiL8QFKEH0iK8ANJEX4gqSp+1Zfe22+/XVq/4oorSuvnnFP+N3jp0qWl9dWrV5fWy4yOjpbW9+/fX1rfs2dPaf3BBx88656m68477yytX3jhhT3b92zAmR9IivADSRF+ICnCDyRF+IGkCD+QFOEHknJE9G1njUYjms1m3/bXL+Pj5RcvXrhwYWm9uCZCT7T779vLfbfbf7f7bvf9h9dff72r55+JGo2Gms3mtA4sZ34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrf81dgwYIFdbfQ0gUXXFBaf/7550vrl112WVf737RpU8tau+9H7Nixo7S+b9++jnrCBM78QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5BU23F+21slfV3S8Yi4qlg2X9IvJA1LGpF0W0S837s2Z7b33y8/NAcPHiytz507t7S+a9eulrV217afN29eab1bTzzxRMvayZMnS7cdGxsrre/du7ejnjBhOmf+n0paccayuyW9HBGXS3q5eAxgBmkb/ojYK+m9MxavlLStuL9N0i0V9wWgxzp9z78wIo5KUnF7cXUtAeiHnn/gZ3u97abtZrv3cAD6p9PwH7O9SJKK2+OtVoyIzRHRiIjG0NBQh7sDULVOw79L0tri/lpJz1XTDoB+aRt+289I+q2kK2wftr1O0sOSbrZ9UNLNxWMAM0jbcf6IWNOi9JWKe5m12s0Tf80113T1/FdeeWVX29flrbfeKq3v2bOntN7uuKIc3/ADkiL8QFKEH0iK8ANJEX4gKcIPJMWlu9FTZT/bfeCBB0q3bTeFd9nPhdEeZ34gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIpxfvTUPffc07LWbgruW2+9tbR+++23d9QTJnDmB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOdHVw4dOlRaf+SRR1rW2v1e/9577y2tz5kzp7SOcpz5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCptuP8trdK+rqk4xFxVbHsPknfkjRWrLYxIl7qVZOoz+joaGn96quvLq1HRMvapk2bSrddsmRJaR3dmc6Z/6eSVkyx/EcRsaz4R/CBGaZt+CNir6T3+tALgD7q5j3/Btu/t73V9kWVdQSgLzoN/48lfUnSMklHJf2g1Yq219tu2m6OjY21Wg1An3UU/og4FhGfRMQpST+RtLxk3c0R0YiIxtDQUKd9AqhYR+G3vWjSw29IerOadgD0y3SG+p6RdKOkBbYPS/q+pBttL5MUkkYkfbuHPQLogbbhj4g1Uyze0oNeUIPx8fHS+v33319aP3HiRGn92muvbVlbt25d6bboLb7hByRF+IGkCD+QFOEHkiL8QFKEH0iKS3fPch9++GFpfenSpaX1br+SvXv37pa1efPmdfXc6A5nfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IinH+WW7btm2l9Xbj+O0uzf3oo4+W1hnLH1yc+YGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcb5Z4GPP/64ZW3Dhg2l25533nml9Yceeqi0fsMNN5TWMbg48wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUm3H+W0vlvS0pM9LOiVpc0Q8Znu+pF9IGpY0Ium2iHi/d63m9dFHH5XWV69e3bJmu3Tbxx9/vLS+YsWK0jpmrumc+U9K+l5E/J2k6yR9x/YSSXdLejkiLpf0cvEYwAzRNvwRcTQiXivun5B0QNIlklZKOn2ZmG2SbulVkwCqd1bv+W0PS/qypN9JWhgRR6WJPxCSLq66OQC9M+3w254naYek70bEH89iu/W2m7ab3c77BqA60wq/7c9oIvjbI2JnsfiY7UVFfZGk41NtGxGbI6IREY2hoaEqegZQgbbh98THxVskHYiIH04q7ZK0tri/VtJz1bcHoFem85Pe6yV9U9I+228UyzZKeljSL22vk/QHSat60yK2b99eWn/xxRdb1hYtWlS67Zo1azrqCTNf2/BHxG8ktRos/kq17QDoF77hByRF+IGkCD+QFOEHkiL8QFKEH0iKS3fPADt37my/UgsvvPBCaf3888/v+Lkxs3HmB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkGOcfAB988EFpff/+/aX1iGhZW7ZsWUc9YfbjzA8kRfiBpAg/kBThB5Ii/EBShB9IivADSTHOPwDOPffc0vr8+fNL66Ojo1W2gyQ48wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUm3H+W0vlvS0pM9LOiVpc0Q8Zvs+Sd+SNFasujEiXupVo7NZu2vnP/XUU6X1m266qWXtySefLN32jjvuKK1j9prOl3xOSvpeRLxm+3OSXrW9u6j9KCI29a49AL3SNvwRcVTS0eL+CdsHJF3S68YA9NZZvee3PSzpy5J+VyzaYPv3trfavqjFNuttN203x8bGploFQA2mHX7b8yTtkPTdiPijpB9L+pKkZZp4ZfCDqbaLiM0R0YiIxtDQUAUtA6jCtMJv+zOaCP72iNgpSRFxLCI+iYhTkn4iaXnv2gRQtbbht21JWyQdiIgfTlq+aNJq35D0ZvXtAeiV6Xzaf72kb0raZ/uNYtlGSWtsL5MUkkYkfbsnHULLl5e/qFq1alXL2l133VW67XXXXVda59Lfs9d0Pu3/jSRPUWJMH5jB+IYfkBThB5Ii/EBShB9IivADSRF+ICku3T0LbNmypaMacuPMDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJOSL6tzN7TNL/TVq0QNJ43xo4O4Pa26D2JdFbp6rs7bKImNb18voa/k/t3G5GRKO2BkoMam+D2pdEb52qqzde9gNJEX4gqbrDv7nm/ZcZ1N4GtS+J3jpVS2+1vucHUJ+6z/wAalJL+G2vsP2/tt+xfXcdPbRie8T2Pttv2G7W3MtW28dtvzlp2Xzbu20fLG6nnCatpt7usz1aHLs3bP9zTb0ttv3ftg/Y3m/7X4vltR67kr5qOW59f9lve46ktyXdLOmwpFckrYmI/+lrIy3YHpHUiIjax4Rt3yDpT5KejoirimWPSHovIh4u/nBeFBH/NiC93SfpT3XP3FxMKLNo8szSkm6R9C+q8diV9HWbajhudZz5l0t6JyLejYg/S/q5pJU19DHwImKvpPfOWLxS0rbi/jZN/M/Tdy16GwgRcTQiXivun5B0embpWo9dSV+1qCP8l0g6NOnxYQ3WlN8h6Ve2X7W9vu5mprCwmDb99PTpF9fcz5naztzcT2fMLD0wx66TGa+rVkf4p5r9Z5CGHK6PiH+Q9DVJ3yle3mJ6pjVzc79MMbP0QOh0xuuq1RH+w5IWT3r8BUlHauhjShFxpLg9LulZDd7sw8dOT5Ja3B6vuZ+/GKSZm6eaWVoDcOwGacbrOsL/iqTLbX/R9mclrZa0q4Y+PsX23OKDGNmeK+mrGrzZh3dJWlvcXyvpuRp7+SuDMnNzq5mlVfOxG7QZr2v5kk8xlPGopDmStkbEg31vYgq2/1YTZ3tp4srGP6uzN9vPSLpRE7/6Oibp+5L+Q9IvJV0q6Q+SVkVE3z94a9HbjZp46fqXmZtPv8fuc2//KOnXkvZJOlUs3qiJ99e1HbuSvtaohuPGN/yApPiGH5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4fCLLFT9WwTz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "We'll need to define 4 parameters, it is really (really) hard to know what good parameter values are on a data set for which you have no experience with, however since MNIST is pretty famous, we have some reasonable values for our data below. The parameters here are:\n",
    "\n",
    "* Learning Rate - How quickly to adjust the cost function.\n",
    "* Training Epochs - How many training cycles to go through\n",
    "* Batch Size - Size of the 'batches' of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "training_epochs = 1000\n",
    "batch_size = 100 #reduced to 20 from 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters\n",
    "\n",
    "Here we have parameters which will directly define our Neural Network, these would be adjusted depending on what your data looked like and what kind of a net you would want to build. Basically just some numbers we will eventually use to define some variables later on in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_hidden_3 = 256 # 3rd layer number of features\n",
    "n_hidden_4 = 256 # 4th layer number of features\n",
    "n_hidden_5 = 256 # 5th layer number of features\n",
    "n_hidden_6 = 256 # 6th layer number of features\n",
    "n_hidden_7 = 256 # 7th layer number of features\n",
    "n_hidden_8 = 256 # 8th layer number of features\n",
    "n_hidden_9 = 256 # 9th layer number of features\n",
    "n_hidden_10 = 256 # 10 layer number of features\n",
    "n_hidden_11 = 256 # 11st layer number of features\n",
    "n_hidden_12 = 256 # 12nd layer number of features\n",
    "n_hidden_13 = 256 # 13rd layer number of features\n",
    "n_hidden_14 = 256 # 14th layer number of features\n",
    "n_hidden_15 = 256 # 15th layer number of features\n",
    "n_hidden_16 = 256 # 16th layer number of features\n",
    "n_hidden_17 = 256 # 17th layer number of features\n",
    "n_hidden_18 = 256 # 18th layer number of features\n",
    "n_hidden_19 = 256 # 19th layer number of features\n",
    "n_hidden_20 = 256 # 20 layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TensorFlow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Model\n",
    "\n",
    "It is time to create our model, let's review what we want to create here.\n",
    "\n",
    "First we receive the input data array and then to send it to the first hidden layer. Then the data will begin to have a weight attached to it between layers (remember this is initially a random value) and then sent to a node to undergo an activation function (along with a Bias as mentioned in the lecture). Then it will continue on to the next hidden layer, and so on until the final output layer. In our case, we will just use two hidden layers, the more you use the longer the model will take to run (but it has more of an opportunity to possibly be more accurate on the training data).\n",
    "\n",
    "Once the transformed \"data\" has reached the output layer we need to evaluate it. Here we will use a loss function (also called a cost function) to evaluate how far off we are from the desired result. In this case, how many of the classes we got correct. \n",
    "\n",
    "Then we will apply an optimization function to minimize the cost (lower the error). This is done by adjusting weight values accordingly across the network. In out example, we will use the [Adam Optimizer](http://arxiv.org/pdf/1412.6980v8.pdf), which keep in mind, relative to other mathematical concepts, is an extremely recent development.\n",
    "\n",
    "We can adjust how quickly to apply this optimization by changing our earlier learning rate parameter. The lower the rate the higher the possibility for accurate training results, but that comes at the cost of having to wait (physical time wise) for the results. Of course, after a certain point there is no benefit to lower the learning rate.\n",
    "\n",
    "Now we will create our model, we'll start with 2 hidden layers, which use the [RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation function, which is a very simple rectifier function which essentially either returns x or zero. For our final output layer we will use a linear activation with matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    '''\n",
    "    x : Place Holder for Data Input\n",
    "    weights: Dictionary of weights\n",
    "    biases: Dicitionary of biases\n",
    "    '''\n",
    "    \n",
    "    # First Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])   # X * W +B\n",
    "    layer_1 = tf.nn.relu(layer_1)  # Func(X * W + B) = RELU --> f(x) = max(o,x)\n",
    "    \n",
    "    # Second Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias\n",
    "\n",
    "In order for our tensorflow model to work we need to create two dictionaries containing our weight and bias objects for the model. We can use the **tf.variable** object type. This is different from a constant because TensorFlow's Graph Object becomes aware of the states of all the variables. A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. We will generally have the model parameters be Variables. From the documentation string:\n",
    "\n",
    "    A variable maintains state in the graph across calls to `run()`. You add a variable to the graph by constructing an instance of the class `Variable`.\n",
    "\n",
    "    The `Variable()` constructor requires an initial value for the variable, which can be a `Tensor` of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n",
    "    \n",
    "We'll use tf's built-in random_normal method to create the random values for our weights and biases (you could also just pass ones as the initial biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "    'h5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5])),\n",
    "    'h6': tf.Variable(tf.random_normal([n_hidden_5, n_hidden_6])),\n",
    "    'h7': tf.Variable(tf.random_normal([n_hidden_6, n_hidden_7])),\n",
    "    'h8': tf.Variable(tf.random_normal([n_hidden_7, n_hidden_8])),\n",
    "    'h9': tf.Variable(tf.random_normal([n_hidden_8, n_hidden_9])),    \n",
    "    'h10': tf.Variable(tf.random_normal([n_hidden_9, n_hidden_10])),\n",
    "    'h11': tf.Variable(tf.random_normal([n_hidden_10, n_hidden_11])),\n",
    "    'h12': tf.Variable(tf.random_normal([n_hidden_11, n_hidden_12])),\n",
    "    'h13': tf.Variable(tf.random_normal([n_hidden_12, n_hidden_13])),\n",
    "    'h14': tf.Variable(tf.random_normal([n_hidden_13, n_hidden_14])),\n",
    "    'h15': tf.Variable(tf.random_normal([n_hidden_14, n_hidden_15])),\n",
    "    'h16': tf.Variable(tf.random_normal([n_hidden_15, n_hidden_16])),\n",
    "    'h17': tf.Variable(tf.random_normal([n_hidden_16, n_hidden_17])),\n",
    "    'h18': tf.Variable(tf.random_normal([n_hidden_17, n_hidden_18])),\n",
    "    'h19': tf.Variable(tf.random_normal([n_hidden_18, n_hidden_19])),    \n",
    "    'h20': tf.Variable(tf.random_normal([n_hidden_19, n_hidden_20])),   \n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_20, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'b5': tf.Variable(tf.random_normal([n_hidden_5])),\n",
    "    'b6': tf.Variable(tf.random_normal([n_hidden_6])),\n",
    "    'b7': tf.Variable(tf.random_normal([n_hidden_7])),\n",
    "    'b8': tf.Variable(tf.random_normal([n_hidden_8])),\n",
    "    'b9': tf.Variable(tf.random_normal([n_hidden_9])),\n",
    "    'b10': tf.Variable(tf.random_normal([n_hidden_10])),\n",
    "    'b11': tf.Variable(tf.random_normal([n_hidden_11])),\n",
    "    'b12': tf.Variable(tf.random_normal([n_hidden_12])),\n",
    "    'b13': tf.Variable(tf.random_normal([n_hidden_13])),\n",
    "    'b14': tf.Variable(tf.random_normal([n_hidden_14])),\n",
    "    'b15': tf.Variable(tf.random_normal([n_hidden_15])),\n",
    "    'b16': tf.Variable(tf.random_normal([n_hidden_16])),\n",
    "    'b17': tf.Variable(tf.random_normal([n_hidden_17])),\n",
    "    'b18': tf.Variable(tf.random_normal([n_hidden_18])),\n",
    "    'b19': tf.Variable(tf.random_normal([n_hidden_19])),\n",
    "    'b20': tf.Variable(tf.random_normal([n_hidden_20])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None,n_input]) # Missing in class notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder('float', [None,n_classes]) # Missing in class notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost and Optimization Functions\n",
    "\n",
    "We'll use Tensorflow's built-in functions for this part (check out the documentation for a lot more options and discussion on this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) # New version tf - changed pred,y to logits=pred, labels=y\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of Variables\n",
    "\n",
    "Now initialize all those tf.Variable objects we created earlier. This will be the first thing we run when training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "#init = tf.initialize_all_variables() # Updated for be version of tf\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "### next_batch()\n",
    "\n",
    "Before we get started I want to cover one more convenience function in our mnist data object called next_batch. This returns a tuple in the form (X,y) with an array of the data and a y array indicating the class in the form of a binary array. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsamp,ysamp = mnist.train.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a63986a208>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Xsamp.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Remember indexing starts at zero!\n",
    "print(ysamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close() #InteractiveSession.close() # UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Session\n",
    "Now it is time to run our session! Pay attention to how we have two loops, the outer loop which runs the epochs, and the inner loop which runs the batches for each epoch of training. Let's breakdown each step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=1515.7135\n",
      "Epoch: 2 cost=1159.8818\n",
      "Epoch: 3 cost=933.6305\n",
      "Epoch: 4 cost=775.5188\n",
      "Epoch: 5 cost=653.0583\n",
      "Epoch: 6 cost=554.4215\n",
      "Epoch: 7 cost=474.5736\n",
      "Epoch: 8 cost=410.7989\n",
      "Epoch: 9 cost=360.7040\n",
      "Epoch: 10 cost=321.0000\n",
      "Epoch: 11 cost=289.0410\n",
      "Epoch: 12 cost=262.9439\n",
      "Epoch: 13 cost=241.3270\n",
      "Epoch: 14 cost=223.0795\n",
      "Epoch: 15 cost=207.6693\n",
      "Epoch: 16 cost=194.4100\n",
      "Epoch: 17 cost=182.8887\n",
      "Epoch: 18 cost=172.7001\n",
      "Epoch: 19 cost=163.6956\n",
      "Epoch: 20 cost=155.6604\n",
      "Epoch: 21 cost=148.4198\n",
      "Epoch: 22 cost=141.8960\n",
      "Epoch: 23 cost=136.0030\n",
      "Epoch: 24 cost=130.6372\n",
      "Epoch: 25 cost=125.7302\n",
      "Epoch: 26 cost=121.1963\n",
      "Epoch: 27 cost=117.0630\n",
      "Epoch: 28 cost=113.2394\n",
      "Epoch: 29 cost=109.6706\n",
      "Epoch: 30 cost=106.3733\n",
      "Epoch: 31 cost=103.2977\n",
      "Epoch: 32 cost=100.4158\n",
      "Epoch: 33 cost=97.7138\n",
      "Epoch: 34 cost=95.1945\n",
      "Epoch: 35 cost=92.8197\n",
      "Epoch: 36 cost=90.5924\n",
      "Epoch: 37 cost=88.4440\n",
      "Epoch: 38 cost=86.4469\n",
      "Epoch: 39 cost=84.5449\n",
      "Epoch: 40 cost=82.7466\n",
      "Epoch: 41 cost=81.0483\n",
      "Epoch: 42 cost=79.4206\n",
      "Epoch: 43 cost=77.8608\n",
      "Epoch: 44 cost=76.3828\n",
      "Epoch: 45 cost=74.9609\n",
      "Epoch: 46 cost=73.5614\n",
      "Epoch: 47 cost=72.2762\n",
      "Epoch: 48 cost=70.9592\n",
      "Epoch: 49 cost=69.7640\n",
      "Epoch: 50 cost=68.5929\n",
      "Epoch: 51 cost=67.4706\n",
      "Epoch: 52 cost=66.3697\n",
      "Epoch: 53 cost=65.3074\n",
      "Epoch: 54 cost=64.2972\n",
      "Epoch: 55 cost=63.3071\n",
      "Epoch: 56 cost=62.3508\n",
      "Epoch: 57 cost=61.4136\n",
      "Epoch: 58 cost=60.5057\n",
      "Epoch: 59 cost=59.6443\n",
      "Epoch: 60 cost=58.7744\n",
      "Epoch: 61 cost=57.9702\n",
      "Epoch: 62 cost=57.1761\n",
      "Epoch: 63 cost=56.4096\n",
      "Epoch: 64 cost=55.6433\n",
      "Epoch: 65 cost=54.9146\n",
      "Epoch: 66 cost=54.2073\n",
      "Epoch: 67 cost=53.5283\n",
      "Epoch: 68 cost=52.8060\n",
      "Epoch: 69 cost=52.1655\n",
      "Epoch: 70 cost=51.5188\n",
      "Epoch: 71 cost=50.8880\n",
      "Epoch: 72 cost=50.2700\n",
      "Epoch: 73 cost=49.6766\n",
      "Epoch: 74 cost=49.0609\n",
      "Epoch: 75 cost=48.4898\n",
      "Epoch: 76 cost=47.9449\n",
      "Epoch: 77 cost=47.3446\n",
      "Epoch: 78 cost=46.8169\n",
      "Epoch: 79 cost=46.2836\n",
      "Epoch: 80 cost=45.7583\n",
      "Epoch: 81 cost=45.2651\n",
      "Epoch: 82 cost=44.7526\n",
      "Epoch: 83 cost=44.2775\n",
      "Epoch: 84 cost=43.8043\n",
      "Epoch: 85 cost=43.3535\n",
      "Epoch: 86 cost=42.8533\n",
      "Epoch: 87 cost=42.4296\n",
      "Epoch: 88 cost=41.9916\n",
      "Epoch: 89 cost=41.5578\n",
      "Epoch: 90 cost=41.1358\n",
      "Epoch: 91 cost=40.7216\n",
      "Epoch: 92 cost=40.3094\n",
      "Epoch: 93 cost=39.9272\n",
      "Epoch: 94 cost=39.4961\n",
      "Epoch: 95 cost=39.1244\n",
      "Epoch: 96 cost=38.7475\n",
      "Epoch: 97 cost=38.3705\n",
      "Epoch: 98 cost=38.0020\n",
      "Epoch: 99 cost=37.6492\n",
      "Epoch: 100 cost=37.2695\n",
      "Epoch: 101 cost=36.9268\n",
      "Epoch: 102 cost=36.5738\n",
      "Epoch: 103 cost=36.2331\n",
      "Epoch: 104 cost=35.9024\n",
      "Epoch: 105 cost=35.5683\n",
      "Epoch: 106 cost=35.2472\n",
      "Epoch: 107 cost=34.9390\n",
      "Epoch: 108 cost=34.5942\n",
      "Epoch: 109 cost=34.2949\n",
      "Epoch: 110 cost=33.9968\n",
      "Epoch: 111 cost=33.6826\n",
      "Epoch: 112 cost=33.3950\n",
      "Epoch: 113 cost=33.1041\n",
      "Epoch: 114 cost=32.8120\n",
      "Epoch: 115 cost=32.5377\n",
      "Epoch: 116 cost=32.2418\n",
      "Epoch: 117 cost=31.9769\n",
      "Epoch: 118 cost=31.7086\n",
      "Epoch: 119 cost=31.4404\n",
      "Epoch: 120 cost=31.1740\n",
      "Epoch: 121 cost=30.9176\n",
      "Epoch: 122 cost=30.6644\n",
      "Epoch: 123 cost=30.4159\n",
      "Epoch: 124 cost=30.1603\n",
      "Epoch: 125 cost=29.9171\n",
      "Epoch: 126 cost=29.6770\n",
      "Epoch: 127 cost=29.4321\n",
      "Epoch: 128 cost=29.1939\n",
      "Epoch: 129 cost=28.9603\n",
      "Epoch: 130 cost=28.7299\n",
      "Epoch: 131 cost=28.5036\n",
      "Epoch: 132 cost=28.2773\n",
      "Epoch: 133 cost=28.0555\n",
      "Epoch: 134 cost=27.8351\n",
      "Epoch: 135 cost=27.6168\n",
      "Epoch: 136 cost=27.4051\n",
      "Epoch: 137 cost=27.1951\n",
      "Epoch: 138 cost=26.9822\n",
      "Epoch: 139 cost=26.7779\n",
      "Epoch: 140 cost=26.5746\n",
      "Epoch: 141 cost=26.3698\n",
      "Epoch: 142 cost=26.1779\n",
      "Epoch: 143 cost=25.9724\n",
      "Epoch: 144 cost=25.7843\n",
      "Epoch: 145 cost=25.5900\n",
      "Epoch: 146 cost=25.3973\n",
      "Epoch: 147 cost=25.2151\n",
      "Epoch: 148 cost=25.0300\n",
      "Epoch: 149 cost=24.8522\n",
      "Epoch: 150 cost=24.6595\n",
      "Epoch: 151 cost=24.4841\n",
      "Epoch: 152 cost=24.3107\n",
      "Epoch: 153 cost=24.1336\n",
      "Epoch: 154 cost=23.9608\n",
      "Epoch: 155 cost=23.7895\n",
      "Epoch: 156 cost=23.6189\n",
      "Epoch: 157 cost=23.4569\n",
      "Epoch: 158 cost=23.2902\n",
      "Epoch: 159 cost=23.1344\n",
      "Epoch: 160 cost=22.9633\n",
      "Epoch: 161 cost=22.8101\n",
      "Epoch: 162 cost=22.6460\n",
      "Epoch: 163 cost=22.4941\n",
      "Epoch: 164 cost=22.3329\n",
      "Epoch: 165 cost=22.1802\n",
      "Epoch: 166 cost=22.0312\n",
      "Epoch: 167 cost=21.8820\n",
      "Epoch: 168 cost=21.7352\n",
      "Epoch: 169 cost=21.5882\n",
      "Epoch: 170 cost=21.4397\n",
      "Epoch: 171 cost=21.2959\n",
      "Epoch: 172 cost=21.1666\n",
      "Epoch: 173 cost=21.0024\n",
      "Epoch: 174 cost=20.8824\n",
      "Epoch: 175 cost=20.7275\n",
      "Epoch: 176 cost=20.5976\n",
      "Epoch: 177 cost=20.4621\n",
      "Epoch: 178 cost=20.3268\n",
      "Epoch: 179 cost=20.1916\n",
      "Epoch: 180 cost=20.0601\n",
      "Epoch: 181 cost=19.9292\n",
      "Epoch: 182 cost=19.8027\n",
      "Epoch: 183 cost=19.6722\n",
      "Epoch: 184 cost=19.5483\n",
      "Epoch: 185 cost=19.4194\n",
      "Epoch: 186 cost=19.2994\n",
      "Epoch: 187 cost=19.1740\n",
      "Epoch: 188 cost=19.0490\n",
      "Epoch: 189 cost=18.9325\n",
      "Epoch: 190 cost=18.8107\n",
      "Epoch: 191 cost=18.6902\n",
      "Epoch: 192 cost=18.5720\n",
      "Epoch: 193 cost=18.4553\n",
      "Epoch: 194 cost=18.3365\n",
      "Epoch: 195 cost=18.2211\n",
      "Epoch: 196 cost=18.0991\n",
      "Epoch: 197 cost=17.9903\n",
      "Epoch: 198 cost=17.8893\n",
      "Epoch: 199 cost=17.7516\n",
      "Epoch: 200 cost=17.6627\n",
      "Epoch: 201 cost=17.5319\n",
      "Epoch: 202 cost=17.4287\n",
      "Epoch: 203 cost=17.3250\n",
      "Epoch: 204 cost=17.2153\n",
      "Epoch: 205 cost=17.1085\n",
      "Epoch: 206 cost=17.0044\n",
      "Epoch: 207 cost=16.9000\n",
      "Epoch: 208 cost=16.7940\n",
      "Epoch: 209 cost=16.6917\n",
      "Epoch: 210 cost=16.5901\n",
      "Epoch: 211 cost=16.4883\n",
      "Epoch: 212 cost=16.3903\n",
      "Epoch: 213 cost=16.2889\n",
      "Epoch: 214 cost=16.1908\n",
      "Epoch: 215 cost=16.0932\n",
      "Epoch: 216 cost=15.9965\n",
      "Epoch: 217 cost=15.9066\n",
      "Epoch: 218 cost=15.7977\n",
      "Epoch: 219 cost=15.7101\n",
      "Epoch: 220 cost=15.6156\n",
      "Epoch: 221 cost=15.5224\n",
      "Epoch: 222 cost=15.4340\n",
      "Epoch: 223 cost=15.3391\n",
      "Epoch: 224 cost=15.2487\n",
      "Epoch: 225 cost=15.1611\n",
      "Epoch: 226 cost=15.0727\n",
      "Epoch: 227 cost=14.9837\n",
      "Epoch: 228 cost=14.8970\n",
      "Epoch: 229 cost=14.8128\n",
      "Epoch: 230 cost=14.7260\n",
      "Epoch: 231 cost=14.6381\n",
      "Epoch: 232 cost=14.5595\n",
      "Epoch: 233 cost=14.4678\n",
      "Epoch: 234 cost=14.3893\n",
      "Epoch: 235 cost=14.3030\n",
      "Epoch: 236 cost=14.2214\n",
      "Epoch: 237 cost=14.1380\n",
      "Epoch: 238 cost=14.0590\n",
      "Epoch: 239 cost=13.9789\n",
      "Epoch: 240 cost=13.8965\n",
      "Epoch: 241 cost=13.8184\n",
      "Epoch: 242 cost=13.7406\n",
      "Epoch: 243 cost=13.6608\n",
      "Epoch: 244 cost=13.5871\n",
      "Epoch: 245 cost=13.5096\n",
      "Epoch: 246 cost=13.4345\n",
      "Epoch: 247 cost=13.3560\n",
      "Epoch: 248 cost=13.2823\n",
      "Epoch: 249 cost=13.2043\n",
      "Epoch: 250 cost=13.1327\n",
      "Epoch: 251 cost=13.0565\n",
      "Epoch: 252 cost=12.9815\n",
      "Epoch: 253 cost=12.9131\n",
      "Epoch: 254 cost=12.8408\n",
      "Epoch: 255 cost=12.7645\n",
      "Epoch: 256 cost=12.6930\n",
      "Epoch: 257 cost=12.6231\n",
      "Epoch: 258 cost=12.5527\n",
      "Epoch: 259 cost=12.4892\n",
      "Epoch: 260 cost=12.4056\n",
      "Epoch: 261 cost=12.3410\n",
      "Epoch: 262 cost=12.2734\n",
      "Epoch: 263 cost=12.2051\n",
      "Epoch: 264 cost=12.1383\n",
      "Epoch: 265 cost=12.0699\n",
      "Epoch: 266 cost=12.0033\n",
      "Epoch: 267 cost=11.9374\n",
      "Epoch: 268 cost=11.8719\n",
      "Epoch: 269 cost=11.8048\n",
      "Epoch: 270 cost=11.7414\n",
      "Epoch: 271 cost=11.6736\n",
      "Epoch: 272 cost=11.6112\n",
      "Epoch: 273 cost=11.5492\n",
      "Epoch: 274 cost=11.4844\n",
      "Epoch: 275 cost=11.4179\n",
      "Epoch: 276 cost=11.3589\n",
      "Epoch: 277 cost=11.2932\n",
      "Epoch: 278 cost=11.2323\n",
      "Epoch: 279 cost=11.1696\n",
      "Epoch: 280 cost=11.1050\n",
      "Epoch: 281 cost=11.0457\n",
      "Epoch: 282 cost=10.9825\n",
      "Epoch: 283 cost=10.9237\n",
      "Epoch: 284 cost=10.8651\n",
      "Epoch: 285 cost=10.8105\n",
      "Epoch: 286 cost=10.7337\n",
      "Epoch: 287 cost=10.6819\n",
      "Epoch: 288 cost=10.6230\n",
      "Epoch: 289 cost=10.5645\n",
      "Epoch: 290 cost=10.5043\n",
      "Epoch: 291 cost=10.4463\n",
      "Epoch: 292 cost=10.3934\n",
      "Epoch: 293 cost=10.3308\n",
      "Epoch: 294 cost=10.2769\n",
      "Epoch: 295 cost=10.2114\n",
      "Epoch: 296 cost=10.1570\n",
      "Epoch: 297 cost=10.1042\n",
      "Epoch: 298 cost=10.0441\n",
      "Epoch: 299 cost=9.9885\n",
      "Epoch: 300 cost=9.9327\n",
      "Epoch: 301 cost=9.8768\n",
      "Epoch: 302 cost=9.8235\n",
      "Epoch: 303 cost=9.7651\n",
      "Epoch: 304 cost=9.7124\n",
      "Epoch: 305 cost=9.6584\n",
      "Epoch: 306 cost=9.6028\n",
      "Epoch: 307 cost=9.5493\n",
      "Epoch: 308 cost=9.4934\n",
      "Epoch: 309 cost=9.4426\n",
      "Epoch: 310 cost=9.3890\n",
      "Epoch: 311 cost=9.3386\n",
      "Epoch: 312 cost=9.2868\n",
      "Epoch: 313 cost=9.2332\n",
      "Epoch: 314 cost=9.1804\n",
      "Epoch: 315 cost=9.1313\n",
      "Epoch: 316 cost=9.0783\n",
      "Epoch: 317 cost=9.0283\n",
      "Epoch: 318 cost=8.9770\n",
      "Epoch: 319 cost=8.9269\n",
      "Epoch: 320 cost=8.8837\n",
      "Epoch: 321 cost=8.8237\n",
      "Epoch: 322 cost=8.7785\n",
      "Epoch: 323 cost=8.7293\n",
      "Epoch: 324 cost=8.6807\n",
      "Epoch: 325 cost=8.6347\n",
      "Epoch: 326 cost=8.5835\n",
      "Epoch: 327 cost=8.5356\n",
      "Epoch: 328 cost=8.4985\n",
      "Epoch: 329 cost=8.4308\n",
      "Epoch: 330 cost=8.3931\n",
      "Epoch: 331 cost=8.3491\n",
      "Epoch: 332 cost=8.2999\n",
      "Epoch: 333 cost=8.2523\n",
      "Epoch: 334 cost=8.2054\n",
      "Epoch: 335 cost=8.1603\n",
      "Epoch: 336 cost=8.1114\n",
      "Epoch: 337 cost=8.0682\n",
      "Epoch: 338 cost=8.0203\n",
      "Epoch: 339 cost=7.9754\n",
      "Epoch: 340 cost=7.9297\n",
      "Epoch: 341 cost=7.8840\n",
      "Epoch: 342 cost=7.8416\n",
      "Epoch: 343 cost=7.8032\n",
      "Epoch: 344 cost=7.7423\n",
      "Epoch: 345 cost=7.7082\n",
      "Epoch: 346 cost=7.6620\n",
      "Epoch: 347 cost=7.6185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 348 cost=7.5753\n",
      "Epoch: 349 cost=7.5324\n",
      "Epoch: 350 cost=7.4915\n",
      "Epoch: 351 cost=7.4445\n",
      "Epoch: 352 cost=7.4045\n",
      "Epoch: 353 cost=7.3628\n",
      "Epoch: 354 cost=7.3197\n",
      "Epoch: 355 cost=7.2778\n",
      "Epoch: 356 cost=7.2343\n",
      "Epoch: 357 cost=7.1956\n",
      "Epoch: 358 cost=7.1544\n",
      "Epoch: 359 cost=7.1123\n",
      "Epoch: 360 cost=7.0722\n",
      "Epoch: 361 cost=7.0302\n",
      "Epoch: 362 cost=6.9901\n",
      "Epoch: 363 cost=6.9499\n",
      "Epoch: 364 cost=6.9087\n",
      "Epoch: 365 cost=6.8699\n",
      "Epoch: 366 cost=6.8307\n",
      "Epoch: 367 cost=6.7928\n",
      "Epoch: 368 cost=6.7525\n",
      "Epoch: 369 cost=6.7118\n",
      "Epoch: 370 cost=6.6729\n",
      "Epoch: 371 cost=6.6351\n",
      "Epoch: 372 cost=6.5960\n",
      "Epoch: 373 cost=6.5589\n",
      "Epoch: 374 cost=6.5199\n",
      "Epoch: 375 cost=6.4822\n",
      "Epoch: 376 cost=6.4612\n",
      "Epoch: 377 cost=6.3899\n",
      "Epoch: 378 cost=6.3702\n",
      "Epoch: 379 cost=6.3311\n",
      "Epoch: 380 cost=6.2967\n",
      "Epoch: 381 cost=6.2536\n",
      "Epoch: 382 cost=6.2208\n",
      "Epoch: 383 cost=6.1848\n",
      "Epoch: 384 cost=6.1468\n",
      "Epoch: 385 cost=6.1115\n",
      "Epoch: 386 cost=6.0750\n",
      "Epoch: 387 cost=6.0403\n",
      "Epoch: 388 cost=6.0047\n",
      "Epoch: 389 cost=5.9695\n",
      "Epoch: 390 cost=5.9343\n",
      "Epoch: 391 cost=5.8992\n",
      "Epoch: 392 cost=5.8623\n",
      "Epoch: 393 cost=5.8277\n",
      "Epoch: 394 cost=5.7943\n",
      "Epoch: 395 cost=5.7595\n",
      "Epoch: 396 cost=5.7257\n",
      "Epoch: 397 cost=5.6931\n",
      "Epoch: 398 cost=5.6570\n",
      "Epoch: 399 cost=5.6224\n",
      "Epoch: 400 cost=5.5920\n",
      "Epoch: 401 cost=5.5581\n",
      "Epoch: 402 cost=5.5228\n",
      "Epoch: 403 cost=5.4924\n",
      "Epoch: 404 cost=5.4587\n",
      "Epoch: 405 cost=5.4257\n",
      "Epoch: 406 cost=5.3991\n",
      "Epoch: 407 cost=5.3575\n",
      "Epoch: 408 cost=5.3295\n",
      "Epoch: 409 cost=5.2979\n",
      "Epoch: 410 cost=5.2649\n",
      "Epoch: 411 cost=5.2335\n",
      "Epoch: 412 cost=5.2013\n",
      "Epoch: 413 cost=5.1731\n",
      "Epoch: 414 cost=5.1389\n",
      "Epoch: 415 cost=5.1085\n",
      "Epoch: 416 cost=5.0787\n",
      "Epoch: 417 cost=5.0480\n",
      "Epoch: 418 cost=5.0161\n",
      "Epoch: 419 cost=4.9864\n",
      "Epoch: 420 cost=4.9572\n",
      "Epoch: 421 cost=4.9262\n",
      "Epoch: 422 cost=4.8964\n",
      "Epoch: 423 cost=4.8663\n",
      "Epoch: 424 cost=4.8374\n",
      "Epoch: 425 cost=4.8077\n",
      "Epoch: 426 cost=4.7775\n",
      "Epoch: 427 cost=4.7493\n",
      "Epoch: 428 cost=4.7198\n",
      "Epoch: 429 cost=4.6917\n",
      "Epoch: 430 cost=4.6606\n",
      "Epoch: 431 cost=4.6337\n",
      "Epoch: 432 cost=4.6038\n",
      "Epoch: 433 cost=4.5761\n",
      "Epoch: 434 cost=4.5458\n",
      "Epoch: 435 cost=4.5190\n",
      "Epoch: 436 cost=4.4915\n",
      "Epoch: 437 cost=4.4617\n",
      "Epoch: 438 cost=4.4345\n",
      "Epoch: 439 cost=4.4070\n",
      "Epoch: 440 cost=4.3792\n",
      "Epoch: 441 cost=4.3516\n",
      "Epoch: 442 cost=4.3264\n",
      "Epoch: 443 cost=4.2954\n",
      "Epoch: 444 cost=4.2695\n",
      "Epoch: 445 cost=4.2425\n",
      "Epoch: 446 cost=4.2161\n",
      "Epoch: 447 cost=4.1888\n",
      "Epoch: 448 cost=4.1630\n",
      "Epoch: 449 cost=4.1352\n",
      "Epoch: 450 cost=4.1072\n",
      "Epoch: 451 cost=4.0812\n",
      "Epoch: 452 cost=4.0554\n",
      "Epoch: 453 cost=4.0280\n",
      "Epoch: 454 cost=4.0031\n",
      "Epoch: 455 cost=3.9774\n",
      "Epoch: 456 cost=3.9512\n",
      "Epoch: 457 cost=3.9262\n",
      "Epoch: 458 cost=3.8987\n",
      "Epoch: 459 cost=3.8753\n",
      "Epoch: 460 cost=3.8500\n",
      "Epoch: 461 cost=3.8262\n",
      "Epoch: 462 cost=3.7981\n",
      "Epoch: 463 cost=3.7743\n",
      "Epoch: 464 cost=3.7499\n",
      "Epoch: 465 cost=3.7243\n",
      "Epoch: 466 cost=3.7000\n",
      "Epoch: 467 cost=3.6756\n",
      "Epoch: 468 cost=3.6504\n",
      "Epoch: 469 cost=3.6266\n",
      "Epoch: 470 cost=3.6033\n",
      "Epoch: 471 cost=3.5784\n",
      "Epoch: 472 cost=3.5556\n",
      "Epoch: 473 cost=3.5409\n",
      "Epoch: 474 cost=3.4991\n",
      "Epoch: 475 cost=3.4843\n",
      "Epoch: 476 cost=3.4602\n",
      "Epoch: 477 cost=3.4371\n",
      "Epoch: 478 cost=3.4126\n",
      "Epoch: 479 cost=3.3947\n",
      "Epoch: 480 cost=3.3627\n",
      "Epoch: 481 cost=3.3432\n",
      "Epoch: 482 cost=3.3208\n",
      "Epoch: 483 cost=3.2980\n",
      "Epoch: 484 cost=3.2765\n",
      "Epoch: 485 cost=3.2527\n",
      "Epoch: 486 cost=3.2298\n",
      "Epoch: 487 cost=3.2068\n",
      "Epoch: 488 cost=3.1857\n",
      "Epoch: 489 cost=3.1631\n",
      "Epoch: 490 cost=3.1421\n",
      "Epoch: 491 cost=3.1198\n",
      "Epoch: 492 cost=3.0980\n",
      "Epoch: 493 cost=3.0765\n",
      "Epoch: 494 cost=3.0608\n",
      "Epoch: 495 cost=3.0288\n",
      "Epoch: 496 cost=3.0126\n",
      "Epoch: 497 cost=2.9906\n",
      "Epoch: 498 cost=2.9707\n",
      "Epoch: 499 cost=2.9492\n",
      "Epoch: 500 cost=2.9289\n",
      "Epoch: 501 cost=2.9082\n",
      "Epoch: 502 cost=2.8875\n",
      "Epoch: 503 cost=2.8679\n",
      "Epoch: 504 cost=2.8477\n",
      "Epoch: 505 cost=2.8263\n",
      "Epoch: 506 cost=2.8062\n",
      "Epoch: 507 cost=2.7867\n",
      "Epoch: 508 cost=2.7661\n",
      "Epoch: 509 cost=2.7464\n",
      "Epoch: 510 cost=2.7261\n",
      "Epoch: 511 cost=2.7072\n",
      "Epoch: 512 cost=2.6880\n",
      "Epoch: 513 cost=2.6681\n",
      "Epoch: 514 cost=2.6484\n",
      "Epoch: 515 cost=2.6303\n",
      "Epoch: 516 cost=2.6108\n",
      "Epoch: 517 cost=2.5923\n",
      "Epoch: 518 cost=2.5717\n",
      "Epoch: 519 cost=2.5542\n",
      "Epoch: 520 cost=2.5350\n",
      "Epoch: 521 cost=2.5155\n",
      "Epoch: 522 cost=2.4970\n",
      "Epoch: 523 cost=2.4777\n",
      "Epoch: 524 cost=2.4596\n",
      "Epoch: 525 cost=2.4415\n",
      "Epoch: 526 cost=2.4218\n",
      "Epoch: 527 cost=2.4052\n",
      "Epoch: 528 cost=2.3865\n",
      "Epoch: 529 cost=2.3676\n",
      "Epoch: 530 cost=2.3503\n",
      "Epoch: 531 cost=2.3318\n",
      "Epoch: 532 cost=2.3148\n",
      "Epoch: 533 cost=2.2953\n",
      "Epoch: 534 cost=2.2784\n",
      "Epoch: 535 cost=2.2600\n",
      "Epoch: 536 cost=2.2554\n",
      "Epoch: 537 cost=2.2135\n",
      "Epoch: 538 cost=2.2074\n",
      "Epoch: 539 cost=2.1939\n",
      "Epoch: 540 cost=2.1708\n",
      "Epoch: 541 cost=2.1552\n",
      "Epoch: 542 cost=2.1392\n",
      "Epoch: 543 cost=2.1219\n",
      "Epoch: 544 cost=2.1055\n",
      "Epoch: 545 cost=2.0881\n",
      "Epoch: 546 cost=2.0708\n",
      "Epoch: 547 cost=2.0539\n",
      "Epoch: 548 cost=2.0382\n",
      "Epoch: 549 cost=2.0221\n",
      "Epoch: 550 cost=2.0048\n",
      "Epoch: 551 cost=1.9890\n",
      "Epoch: 552 cost=1.9724\n",
      "Epoch: 553 cost=1.9556\n",
      "Epoch: 554 cost=1.9400\n",
      "Epoch: 555 cost=1.9252\n",
      "Epoch: 556 cost=1.9078\n",
      "Epoch: 557 cost=1.8929\n",
      "Epoch: 558 cost=1.8757\n",
      "Epoch: 559 cost=1.8596\n",
      "Epoch: 560 cost=1.8444\n",
      "Epoch: 561 cost=1.8284\n",
      "Epoch: 562 cost=1.8135\n",
      "Epoch: 563 cost=1.7976\n",
      "Epoch: 564 cost=1.7812\n",
      "Epoch: 565 cost=1.7671\n",
      "Epoch: 566 cost=1.7512\n",
      "Epoch: 567 cost=1.7364\n",
      "Epoch: 568 cost=1.7205\n",
      "Epoch: 569 cost=1.7061\n",
      "Epoch: 570 cost=1.6905\n",
      "Epoch: 571 cost=1.6753\n",
      "Epoch: 572 cost=1.6616\n",
      "Epoch: 573 cost=1.6467\n",
      "Epoch: 574 cost=1.6314\n",
      "Epoch: 575 cost=1.6173\n",
      "Epoch: 576 cost=1.6025\n",
      "Epoch: 577 cost=1.5887\n",
      "Epoch: 578 cost=1.5741\n",
      "Epoch: 579 cost=1.5598\n",
      "Epoch: 580 cost=1.5451\n",
      "Epoch: 581 cost=1.5310\n",
      "Epoch: 582 cost=1.5170\n",
      "Epoch: 583 cost=1.5025\n",
      "Epoch: 584 cost=1.4895\n",
      "Epoch: 585 cost=1.4744\n",
      "Epoch: 586 cost=1.4622\n",
      "Epoch: 587 cost=1.4466\n",
      "Epoch: 588 cost=1.4350\n",
      "Epoch: 589 cost=1.4222\n",
      "Epoch: 590 cost=1.4079\n",
      "Epoch: 591 cost=1.3954\n",
      "Epoch: 592 cost=1.3811\n",
      "Epoch: 593 cost=1.3688\n",
      "Epoch: 594 cost=1.3559\n",
      "Epoch: 595 cost=1.3422\n",
      "Epoch: 596 cost=1.3293\n",
      "Epoch: 597 cost=1.3173\n",
      "Epoch: 598 cost=1.3043\n",
      "Epoch: 599 cost=1.2910\n",
      "Epoch: 600 cost=1.2786\n",
      "Epoch: 601 cost=1.2662\n",
      "Epoch: 602 cost=1.2541\n",
      "Epoch: 603 cost=1.2407\n",
      "Epoch: 604 cost=1.2289\n",
      "Epoch: 605 cost=1.2163\n",
      "Epoch: 606 cost=1.2044\n",
      "Epoch: 607 cost=1.1916\n",
      "Epoch: 608 cost=1.1811\n",
      "Epoch: 609 cost=1.1686\n",
      "Epoch: 610 cost=1.1566\n",
      "Epoch: 611 cost=1.1446\n",
      "Epoch: 612 cost=1.1334\n",
      "Epoch: 613 cost=1.1214\n",
      "Epoch: 614 cost=1.1101\n",
      "Epoch: 615 cost=1.0975\n",
      "Epoch: 616 cost=1.0867\n",
      "Epoch: 617 cost=1.0751\n",
      "Epoch: 618 cost=1.0644\n",
      "Epoch: 619 cost=1.0525\n",
      "Epoch: 620 cost=1.0409\n",
      "Epoch: 621 cost=1.0308\n",
      "Epoch: 622 cost=1.0190\n",
      "Epoch: 623 cost=1.0090\n",
      "Epoch: 624 cost=0.9975\n",
      "Epoch: 625 cost=0.9864\n",
      "Epoch: 626 cost=0.9752\n",
      "Epoch: 627 cost=0.9651\n",
      "Epoch: 628 cost=0.9547\n",
      "Epoch: 629 cost=0.9431\n",
      "Epoch: 630 cost=0.9332\n",
      "Epoch: 631 cost=0.9227\n",
      "Epoch: 632 cost=0.9114\n",
      "Epoch: 633 cost=0.9004\n",
      "Epoch: 634 cost=0.8907\n",
      "Epoch: 635 cost=0.8808\n",
      "Epoch: 636 cost=0.8699\n",
      "Epoch: 637 cost=0.8605\n",
      "Epoch: 638 cost=0.8504\n",
      "Epoch: 639 cost=0.8407\n",
      "Epoch: 640 cost=0.8300\n",
      "Epoch: 641 cost=0.8208\n",
      "Epoch: 642 cost=0.8102\n",
      "Epoch: 643 cost=0.8012\n",
      "Epoch: 644 cost=0.7914\n",
      "Epoch: 645 cost=0.7813\n",
      "Epoch: 646 cost=0.7721\n",
      "Epoch: 647 cost=0.7624\n",
      "Epoch: 648 cost=0.7534\n",
      "Epoch: 649 cost=0.7437\n",
      "Epoch: 650 cost=0.7344\n",
      "Epoch: 651 cost=0.7247\n",
      "Epoch: 652 cost=0.7159\n",
      "Epoch: 653 cost=0.7059\n",
      "Epoch: 654 cost=0.6981\n",
      "Epoch: 655 cost=0.6890\n",
      "Epoch: 656 cost=0.6804\n",
      "Epoch: 657 cost=0.6711\n",
      "Epoch: 658 cost=0.6623\n",
      "Epoch: 659 cost=0.6538\n",
      "Epoch: 660 cost=0.6454\n",
      "Epoch: 661 cost=0.6372\n",
      "Epoch: 662 cost=0.6285\n",
      "Epoch: 663 cost=0.6206\n",
      "Epoch: 664 cost=0.6133\n",
      "Epoch: 665 cost=0.6038\n",
      "Epoch: 666 cost=0.5964\n",
      "Epoch: 667 cost=0.5882\n",
      "Epoch: 668 cost=0.5809\n",
      "Epoch: 669 cost=0.5724\n",
      "Epoch: 670 cost=0.5647\n",
      "Epoch: 671 cost=0.5567\n",
      "Epoch: 672 cost=0.5490\n",
      "Epoch: 673 cost=0.5413\n",
      "Epoch: 674 cost=0.5332\n",
      "Epoch: 675 cost=0.5259\n",
      "Epoch: 676 cost=0.5188\n",
      "Epoch: 677 cost=0.5104\n",
      "Epoch: 678 cost=0.5032\n",
      "Epoch: 679 cost=0.4963\n",
      "Epoch: 680 cost=0.4887\n",
      "Epoch: 681 cost=0.4813\n",
      "Epoch: 682 cost=0.4738\n",
      "Epoch: 683 cost=0.4674\n",
      "Epoch: 684 cost=0.4605\n",
      "Epoch: 685 cost=0.4539\n",
      "Epoch: 686 cost=0.4473\n",
      "Epoch: 687 cost=0.4408\n",
      "Epoch: 688 cost=0.4336\n",
      "Epoch: 689 cost=0.4276\n",
      "Epoch: 690 cost=0.4209\n",
      "Epoch: 691 cost=0.4148\n",
      "Epoch: 692 cost=0.4083\n",
      "Epoch: 693 cost=0.4014\n",
      "Epoch: 694 cost=0.3952\n",
      "Epoch: 695 cost=0.3887\n",
      "Epoch: 696 cost=0.3823\n",
      "Epoch: 697 cost=0.3762\n",
      "Epoch: 698 cost=0.3702\n",
      "Epoch: 699 cost=0.3639\n",
      "Epoch: 700 cost=0.3579\n",
      "Epoch: 701 cost=0.3517\n",
      "Epoch: 702 cost=0.3450\n",
      "Epoch: 703 cost=0.3399\n",
      "Epoch: 704 cost=0.3337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 705 cost=0.3276\n",
      "Epoch: 706 cost=0.3217\n",
      "Epoch: 707 cost=0.3154\n",
      "Epoch: 708 cost=0.3097\n",
      "Epoch: 709 cost=0.3043\n",
      "Epoch: 710 cost=0.2981\n",
      "Epoch: 711 cost=0.2937\n",
      "Epoch: 712 cost=0.2873\n",
      "Epoch: 713 cost=0.2817\n",
      "Epoch: 714 cost=0.2766\n",
      "Epoch: 715 cost=0.2705\n",
      "Epoch: 716 cost=0.2658\n",
      "Epoch: 717 cost=0.2597\n",
      "Epoch: 718 cost=0.2544\n",
      "Epoch: 719 cost=0.2497\n",
      "Epoch: 720 cost=0.2445\n",
      "Epoch: 721 cost=0.2390\n",
      "Epoch: 722 cost=0.2333\n",
      "Epoch: 723 cost=0.2289\n",
      "Epoch: 724 cost=0.2240\n",
      "Epoch: 725 cost=0.2188\n",
      "Epoch: 726 cost=0.2138\n",
      "Epoch: 727 cost=0.2092\n",
      "Epoch: 728 cost=0.2046\n",
      "Epoch: 729 cost=0.1996\n",
      "Epoch: 730 cost=0.1952\n",
      "Epoch: 731 cost=0.1909\n",
      "Epoch: 732 cost=0.1861\n",
      "Epoch: 733 cost=0.1818\n",
      "Epoch: 734 cost=0.1777\n",
      "Epoch: 735 cost=0.1732\n",
      "Epoch: 736 cost=0.1688\n",
      "Epoch: 737 cost=0.1650\n",
      "Epoch: 738 cost=0.1606\n",
      "Epoch: 739 cost=0.1563\n",
      "Epoch: 740 cost=0.1526\n",
      "Epoch: 741 cost=0.1486\n",
      "Epoch: 742 cost=0.1446\n",
      "Epoch: 743 cost=0.1396\n",
      "Epoch: 744 cost=0.1368\n",
      "Epoch: 745 cost=0.1327\n",
      "Epoch: 746 cost=0.1290\n",
      "Epoch: 747 cost=0.1255\n",
      "Epoch: 748 cost=0.1217\n",
      "Epoch: 749 cost=0.1184\n",
      "Epoch: 750 cost=0.1151\n",
      "Epoch: 751 cost=0.1115\n",
      "Epoch: 752 cost=0.1085\n",
      "Epoch: 753 cost=0.1052\n",
      "Epoch: 754 cost=0.1019\n",
      "Epoch: 755 cost=0.0984\n",
      "Epoch: 756 cost=0.0954\n",
      "Epoch: 757 cost=0.0927\n",
      "Epoch: 758 cost=0.0888\n",
      "Epoch: 759 cost=0.0860\n",
      "Epoch: 760 cost=0.0833\n",
      "Epoch: 761 cost=0.0801\n",
      "Epoch: 762 cost=0.0776\n",
      "Epoch: 763 cost=0.0743\n",
      "Epoch: 764 cost=0.0713\n",
      "Epoch: 765 cost=0.0685\n",
      "Epoch: 766 cost=0.0658\n",
      "Epoch: 767 cost=0.0628\n",
      "Epoch: 768 cost=0.0605\n",
      "Epoch: 769 cost=0.0573\n",
      "Epoch: 770 cost=0.0557\n",
      "Epoch: 771 cost=0.0529\n",
      "Epoch: 772 cost=0.0504\n",
      "Epoch: 773 cost=0.0483\n",
      "Epoch: 774 cost=0.0464\n",
      "Epoch: 775 cost=0.0441\n",
      "Epoch: 776 cost=0.0420\n",
      "Epoch: 777 cost=0.0402\n",
      "Epoch: 778 cost=0.0382\n",
      "Epoch: 779 cost=0.0364\n",
      "Epoch: 780 cost=0.0350\n",
      "Epoch: 781 cost=0.0328\n",
      "Epoch: 782 cost=0.0313\n",
      "Epoch: 783 cost=0.0297\n",
      "Epoch: 784 cost=0.0284\n",
      "Epoch: 785 cost=0.0267\n",
      "Epoch: 786 cost=0.0253\n",
      "Epoch: 787 cost=0.0240\n",
      "Epoch: 788 cost=0.0225\n",
      "Epoch: 789 cost=0.0213\n",
      "Epoch: 790 cost=0.0197\n",
      "Epoch: 791 cost=0.0190\n",
      "Epoch: 792 cost=0.0178\n",
      "Epoch: 793 cost=0.0169\n",
      "Epoch: 794 cost=0.0157\n",
      "Epoch: 795 cost=0.0147\n",
      "Epoch: 796 cost=0.0139\n",
      "Epoch: 797 cost=0.0132\n",
      "Epoch: 798 cost=0.0124\n",
      "Epoch: 799 cost=0.0111\n",
      "Epoch: 800 cost=0.0108\n",
      "Epoch: 801 cost=0.0099\n",
      "Epoch: 802 cost=0.0092\n",
      "Epoch: 803 cost=0.0083\n",
      "Epoch: 804 cost=0.0087\n",
      "Epoch: 805 cost=0.0085\n",
      "Epoch: 806 cost=0.0070\n",
      "Epoch: 807 cost=0.0063\n",
      "Epoch: 808 cost=0.0058\n",
      "Epoch: 809 cost=0.0053\n",
      "Epoch: 810 cost=0.0048\n",
      "Epoch: 811 cost=0.0047\n",
      "Epoch: 812 cost=0.0046\n",
      "Epoch: 813 cost=0.0041\n",
      "Epoch: 814 cost=0.0033\n",
      "Epoch: 815 cost=0.0030\n",
      "Epoch: 816 cost=0.0028\n",
      "Epoch: 817 cost=0.0024\n",
      "Epoch: 818 cost=0.0019\n",
      "Epoch: 819 cost=0.0020\n",
      "Epoch: 820 cost=0.0016\n",
      "Epoch: 821 cost=0.0013\n",
      "Epoch: 822 cost=0.0011\n",
      "Epoch: 823 cost=0.0022\n",
      "Epoch: 824 cost=0.0015\n",
      "Epoch: 825 cost=0.0007\n",
      "Epoch: 826 cost=0.0004\n",
      "Epoch: 827 cost=0.0003\n",
      "Epoch: 828 cost=0.0001\n",
      "Epoch: 829 cost=0.0000\n",
      "Epoch: 830 cost=0.0000\n",
      "Epoch: 831 cost=0.0000\n",
      "Epoch: 832 cost=0.0000\n",
      "Epoch: 833 cost=0.0000\n",
      "Epoch: 834 cost=0.0000\n",
      "Epoch: 835 cost=0.0000\n",
      "Epoch: 836 cost=0.0000\n",
      "Epoch: 837 cost=0.0000\n",
      "Epoch: 838 cost=0.0000\n",
      "Epoch: 839 cost=0.0000\n",
      "Epoch: 840 cost=0.0000\n",
      "Epoch: 841 cost=0.0000\n",
      "Epoch: 842 cost=0.0000\n",
      "Epoch: 843 cost=0.0000\n",
      "Epoch: 844 cost=0.0000\n",
      "Epoch: 845 cost=0.0000\n",
      "Epoch: 846 cost=0.0000\n",
      "Epoch: 847 cost=0.0000\n",
      "Epoch: 848 cost=0.0000\n",
      "Epoch: 849 cost=0.0000\n",
      "Epoch: 850 cost=0.0000\n",
      "Epoch: 851 cost=0.0000\n",
      "Epoch: 852 cost=0.0000\n",
      "Epoch: 853 cost=0.0000\n",
      "Epoch: 854 cost=0.0000\n",
      "Epoch: 855 cost=0.0000\n",
      "Epoch: 856 cost=0.0000\n",
      "Epoch: 857 cost=0.0000\n",
      "Epoch: 858 cost=0.0000\n",
      "Epoch: 859 cost=0.0019\n",
      "Epoch: 860 cost=0.0000\n",
      "Epoch: 861 cost=0.0000\n",
      "Epoch: 862 cost=0.0000\n",
      "Epoch: 863 cost=0.0000\n",
      "Epoch: 864 cost=0.0000\n",
      "Epoch: 865 cost=0.0000\n",
      "Epoch: 866 cost=0.0000\n",
      "Epoch: 867 cost=0.0000\n",
      "Epoch: 868 cost=0.0000\n",
      "Epoch: 869 cost=0.0000\n",
      "Epoch: 870 cost=0.0000\n",
      "Epoch: 871 cost=0.0000\n",
      "Epoch: 872 cost=0.0000\n",
      "Epoch: 873 cost=0.0000\n",
      "Epoch: 874 cost=0.0000\n",
      "Epoch: 875 cost=0.0000\n",
      "Epoch: 876 cost=0.0000\n",
      "Epoch: 877 cost=0.0000\n",
      "Epoch: 878 cost=0.0000\n",
      "Epoch: 879 cost=0.0000\n",
      "Epoch: 880 cost=0.0000\n",
      "Epoch: 881 cost=0.0000\n",
      "Epoch: 882 cost=0.0000\n",
      "Epoch: 883 cost=0.0000\n",
      "Epoch: 884 cost=0.0000\n",
      "Epoch: 885 cost=0.0000\n",
      "Epoch: 886 cost=0.0000\n",
      "Epoch: 887 cost=0.0000\n",
      "Epoch: 888 cost=0.0000\n",
      "Epoch: 889 cost=0.0000\n",
      "Epoch: 890 cost=0.0000\n",
      "Epoch: 891 cost=0.0000\n",
      "Epoch: 892 cost=0.0000\n",
      "Epoch: 893 cost=0.0000\n",
      "Epoch: 894 cost=0.0000\n",
      "Epoch: 895 cost=0.0000\n",
      "Epoch: 896 cost=0.0000\n",
      "Epoch: 897 cost=0.0000\n",
      "Epoch: 898 cost=0.0000\n",
      "Epoch: 899 cost=0.0000\n",
      "Epoch: 900 cost=0.0000\n",
      "Epoch: 901 cost=0.0000\n",
      "Epoch: 902 cost=0.0000\n",
      "Epoch: 903 cost=0.0000\n",
      "Epoch: 904 cost=0.0000\n",
      "Epoch: 905 cost=0.0002\n",
      "Epoch: 906 cost=0.0002\n",
      "Epoch: 907 cost=0.0000\n",
      "Epoch: 908 cost=0.0000\n",
      "Epoch: 909 cost=0.0000\n",
      "Epoch: 910 cost=0.0000\n",
      "Epoch: 911 cost=0.0000\n",
      "Epoch: 912 cost=0.0000\n",
      "Epoch: 913 cost=0.0000\n",
      "Epoch: 914 cost=0.0000\n",
      "Epoch: 915 cost=0.0000\n",
      "Epoch: 916 cost=0.0000\n",
      "Epoch: 917 cost=0.0000\n",
      "Epoch: 918 cost=0.0000\n",
      "Epoch: 919 cost=0.0000\n",
      "Epoch: 920 cost=0.0000\n",
      "Epoch: 921 cost=0.0000\n",
      "Epoch: 922 cost=0.0000\n",
      "Epoch: 923 cost=0.0000\n",
      "Epoch: 924 cost=0.0000\n",
      "Epoch: 925 cost=0.0000\n",
      "Epoch: 926 cost=0.0000\n",
      "Epoch: 927 cost=0.0000\n",
      "Epoch: 928 cost=0.0000\n",
      "Epoch: 929 cost=0.0000\n",
      "Epoch: 930 cost=0.0000\n",
      "Epoch: 931 cost=0.0000\n",
      "Epoch: 932 cost=0.0000\n",
      "Epoch: 933 cost=0.0000\n",
      "Epoch: 934 cost=0.0000\n",
      "Epoch: 935 cost=0.0000\n",
      "Epoch: 936 cost=0.0000\n",
      "Epoch: 937 cost=0.0000\n",
      "Epoch: 938 cost=0.0000\n",
      "Epoch: 939 cost=0.0000\n",
      "Epoch: 940 cost=0.0000\n",
      "Epoch: 941 cost=0.0000\n",
      "Epoch: 942 cost=0.0000\n",
      "Epoch: 943 cost=0.0000\n",
      "Epoch: 944 cost=0.0000\n",
      "Epoch: 945 cost=0.0000\n",
      "Epoch: 946 cost=0.0000\n",
      "Epoch: 947 cost=0.0000\n",
      "Epoch: 948 cost=0.0000\n",
      "Epoch: 949 cost=0.0000\n",
      "Epoch: 950 cost=0.0000\n",
      "Epoch: 951 cost=0.0000\n",
      "Epoch: 952 cost=0.0008\n",
      "Epoch: 953 cost=0.0000\n",
      "Epoch: 954 cost=0.0000\n",
      "Epoch: 955 cost=0.0000\n",
      "Epoch: 956 cost=0.0000\n",
      "Epoch: 957 cost=0.0000\n",
      "Epoch: 958 cost=0.0000\n",
      "Epoch: 959 cost=0.0000\n",
      "Epoch: 960 cost=0.0000\n",
      "Epoch: 961 cost=0.0000\n",
      "Epoch: 962 cost=0.0000\n",
      "Epoch: 963 cost=0.0000\n",
      "Epoch: 964 cost=0.0000\n",
      "Epoch: 965 cost=0.0000\n",
      "Epoch: 966 cost=0.0000\n",
      "Epoch: 967 cost=0.0000\n",
      "Epoch: 968 cost=0.0000\n",
      "Epoch: 969 cost=0.0000\n",
      "Epoch: 970 cost=0.0000\n",
      "Epoch: 971 cost=0.0000\n",
      "Epoch: 972 cost=0.0000\n",
      "Epoch: 973 cost=0.0000\n",
      "Epoch: 974 cost=0.0000\n",
      "Epoch: 975 cost=0.0000\n",
      "Epoch: 976 cost=0.0000\n",
      "Epoch: 977 cost=0.0000\n",
      "Epoch: 978 cost=0.0000\n",
      "Epoch: 979 cost=0.0000\n",
      "Epoch: 980 cost=0.0000\n",
      "Epoch: 981 cost=0.0000\n",
      "Epoch: 982 cost=0.0000\n",
      "Epoch: 983 cost=0.0000\n",
      "Epoch: 984 cost=0.0000\n",
      "Epoch: 985 cost=0.0000\n",
      "Epoch: 986 cost=0.0000\n",
      "Epoch: 987 cost=0.0000\n",
      "Epoch: 988 cost=0.0000\n",
      "Epoch: 989 cost=0.0000\n",
      "Epoch: 990 cost=0.0000\n",
      "Epoch: 991 cost=0.0000\n",
      "Epoch: 992 cost=0.0000\n",
      "Epoch: 993 cost=0.0000\n",
      "Epoch: 994 cost=0.0000\n",
      "Epoch: 995 cost=0.0000\n",
      "Epoch: 996 cost=0.0000\n",
      "Epoch: 997 cost=0.0000\n",
      "Epoch: 998 cost=0.0000\n",
      "Epoch: 999 cost=0.0000\n",
      "Epoch: 1000 cost=0.0000\n",
      "Model has completed 1000 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "# Launch the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Intialize all the variables\n",
    "sess.run(init)\n",
    "\n",
    "# Training Epochs\n",
    "# Essentially the max amount of loops possible before we stop\n",
    "# May stop earlier if cost/loss limit was set\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    # Start with cost = 0.0\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    # Convert total number of batches to integer\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        # Grab the next batch of training data and labels\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Feed dictionary for optimization and loss value\n",
    "        # Returns a tuple, but we only need 'c' the cost\n",
    "        # So we set an underscore as a \"throwaway\"\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close() # InteractiveSession.close() # UserWarning: An interactive session is already active. \n",
    "#This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` \n",
    "#to release resources held by the other session(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations\n",
    "\n",
    "Tensorflow comes with some built-in functions to help evaluate our model, including tf.equal and tf.cast with tf.reduce_mean.\n",
    "\n",
    "**tf.equal()**\n",
    "\n",
    "This is essentially just a check of predictions == y_test. In our case since we know the format of the labels is a 1 in an array of zeroes, we can compare argmax() location of that 1. Remember that **y** here is still that placeholder we created at the very beginning, we will perform a series of operations to get a Tensor that we can eventually fill in the test data for with an evaluation method. What we are currently running will still be empty of test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_2:0\", shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a numerical value for our predictions we will need to use tf.cast to cast the Tensor of booleans back into a Tensor of Floating point values in order to take the mean of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_3:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the tf.reduce_mean function in order to grab the mean of the elements across the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem a little strange, but this accuracy is still a Tensor object. Remember that we still need to pass in our actual test data! Now we can call the MNIST test labels and images and evaluate our accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval() method allows you to directly evaluates this tensor in a `Session` without needing to call tf.sess():mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9277\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94% not too shabby! But this actually isn't anywhere near as good as it could be. Running for more training epochs with this data (around 20,000) can produce accuracy around 99%. But we won't do that here because that will take a very long time to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job!\n",
    "\n",
    "### Extra Credit: See what happens if you try to make this model again with more layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
